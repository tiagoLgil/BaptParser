Nos últimos anos, temos assistido a um contínuo crescimento do número de iniciativas de “informatização” da pesquisa em história, com a utilização de recursos computacionais nas mais diversas temáticas e abordagens. A expansão da internet levou a que um número incomensurável de documentos digitais fosse criado todos os dias,  nascidos digitais ou digitalizados. A avalanche de dados é tamanha que diferentes iniciativas foram criadas para dar conta desta imensidão.
Essas preocupações foram lentamente chegando na pauta da pesquisa em história, tanto no tratamento dos dados online quanto no uso de fontes digitalizadas, cada vez mais frequentes em todos os cantos do mundo. A agenda das big techs agora é também uma realidade para os historiadores, e o apelo aos chamados big data tem sido uma constante, especialmente na Europa e nos Estados Unidos. Devemos embarcar nessa discussão e abraçar os big data, como defendem, por exemplo, os historiadores Guldi e Armitage? Ou devemos nos precaver e avaliar criticamente estas ferramentas, como quer a matemática e ativista Cathy O’Neil? Temos, em nossa formação de historiadores, ferramentas para isso? Esta contribuição visa a trazer alguns elementos para esse debate, que começa a ganhar fôlego no Brasil.

Era uma vez o futuro
Eram os idos de 2012 quando Frédéric Kaplan iniciava o “Venice Time Machine”, um projeto milionário e internacional que previa a digitalização de muitos quilômetros de documentos do Arquivo de Estado de Veneza (entidade parceira do projeto), para posterior criação de um “modelo multidimensional de Veneza e sua evolução ao longo de um período de mais de 1000 anos”. O projeto previa a instalação de grandes scanners no Arquivo e sua operação por engenheiros (caso do próprio Kaplan) para posterior reconhecimento digital automático dos manuscritos. Por meio de elaborados recursos de programação, seria possível a identificação de nomes de pessoas e lugares. Ela acarretaria, quase como uma obviedade, a descoberta de redes de relacionamentos e a (também óbvia) criação de extensos gráficos de redes sociais, mostrando as (certamente simples) conexões entre as pessoas e seus espaços de atuação:
By combining this mass of information, it is possible to reconstruct large segments of the city's past: complete biographies, political dynamics, or even the appearance of buildings and entire neighborhoods. The information extracted from the primary and secondary sources are organized in a semantic graph of linked data and unfolded in space and time in an historical geographical information system

Era um conto de fadas digital. A moderna engenharia da computação agora procurava fazer aquilo que inúmeros historiadores até então não haviam conseguido: varrer uma série de documentos extremamente longa de modo exaustivo. Não se tratava apenas de digitalizar, mas de reconhecer um sem-número de caligrafias diferentes produzidas em uma diversidade de idiomas (lembrando que Veneza era uma potência comercial que mantinha embaixadores em terras longínquas), inclusive o latim e o vêneto. E não se tratava apenas de reconhecer os textos e seu teor, mas obter informações que permitissem o reconhecimento de personagens e suas vidas, posteriormente montadas e contadas (também para o grande público) com o uso de modernos artifícios computacionais. O conto de fadas se fazia ainda mais fantástico, pois o projeto, em seus websites, utilizaram com esmero recursos gráficos como vídeos, mapas coloridos e gráficos animados, além de uma fala impressionante (e impressionista) em uma TED Talk. O futuro enfim havia alcançado os territórios  do passado, e os historiadores estavam prestes a se tornar espectadores  privilegiados da capacidade computacional que os engenheiros eram capazes de aportar.
O conto de fadas, contudo, teve um fim. Era setembro de 2019 e o Arquivo de Estado de Veneza lançava uma nota à imprensa informando a suspensão dos acordos de cooperação então vigentes com o EPFL (École Polytechnique Fédérale de Lausanne, onde Kaplan atuava) e a Universidade Ca’Foscari de Veneza, também parceira. O motivo era a falta de transparência referente às atividades realizadas pela equipe de Lausanne sobre os procedimentos adotados e os resultados obtidos. O Arquivo de Veneza reclamava de uma indesejada hierarquia entre as instituições, a exclusão de seus técnicos dos trabalhos, a falta de discussão sobre os procedimentos de digitalização e catalogação e, finalmente, a transparência sobre as decisões tomadas e a análise dos primeiros resultados. Segundo o então diretor do Arquivo, Gianni Doria, a decisão de encerrar os trabalhos teria sido mútua, após uma série de tratativas de acordos que especificasse uma nova política de interação. A EPFL, contudo, dizia ter sido uma decisão unilateral do Arquivo e que buscaria novos diálogos, o que não se concretizou. 
A decisão do Arquivo de Veneza não destacava apenas a discussão técnica e política sobre as tratativas. Ao final do documento, uma importante observação era feita, no sentido de discutir os termos da parceria “com a convicção que não é suficiente digitalizar os documentos, mesmo com o uso de instrumentos e algoritmos complexos, para compreender a história de Veneza”. Era uma nota com um perfume e brio locais. Mas era também uma nota de resistência a um projeto que tentou colonizar seus documentos com armas ainda desconhecidas e com um poder de fogo não completamente explicado. 
Não se tratava apenas de uma questão contratual ou de falta de comunicação. O que estava em jogo era a noção do que seria a História na cabeça das diferentes equipes. A do Arquivo era uma visão tradicional, mas muito conectada à leitura erudita das fontes. A de Kaplan, aparentemente mais moderna, dispensava o erudito para substituí-lo por um algoritmo. Ambas fundamentalmente, empiristas, mas a da EPFL não se oferecia à crítica e partia da premissa de que os dados se organizariam por si mesmos, independente de uma teoria de fundo; ou, melhor, os dados se exibiriam de uma forma aceitável para todas as leituras de mundo possíveis, tanto de experts quanto do grande público.
O projeto “Venice Time Machine” tomou, nos anos seguintes, um caminho diferente, que merece análise detida, a ser efetuada em outro momento. Mas convém destacar suas implicações políticas, bastante complexas, distantes da aparência de uma solução meramente técnica para problemas de historiadores. O projeto agora se volta para criar focos de “Time Machine” em várias cidades da Europa, sem jamais perder de vista a ideia de que o uso de algoritmos pode “amplificar” as fontes disponíveis.

O canto da sereia
É famosa a passagem da Odisséia que fala sobre o momento em que Ulisses passara com seu navio junto à ilha ao redor da qual abundavam sereias capazes de atrair, com seu belíssimo canto, as embarcações para perto das rochas, naufragando-as. Para desfrutar do canto sem correr riscos, Ulisses tapou o ouvido de seus marinheiros e se fez amarrar ao mastro do navio.
Não há dúvida de que o Transkribus, uma ferramenta de reconhecimento automático de manuscritos, seja comparável a um canto belíssimo. Criada a partir de um projeto da Universidade de Innsbruck, ela é capaz de identificar texto em um documento manuscrito, reconhecer as linhas e, finalmente, transcrever o texto, identificando uma “zona” da imagem (através de coordenadas cartesianas dos pixels da imagem) com um certo trecho de texto, através da HTR (Handwriting text recognition, ou reconhecimento de texto manuscrito). 
O projeto começou com uma experiência prévia de Günter Mühlberger na digitalização e reconhecimento de caracteres com tecnologia OCR (Optical Character Recognition) em jornais alemães dos anos 1990. A partir daí, a equipe ao redor de Mühlberger foi rumando para novos desafios, e, no início dos anos 2000, já estavam trabalhando em projetos com reconhecimento de manuscritos, ainda que com diversos revezes.
Talvez seja importante explicar a diferença entre as duas tecnologias. Enquantos os sistemas OCR (reconhecimento óptico de caracteres) se valem da semelhança das letras com base em uma padronização muito restrita, como é a dos impressos, para a qual são utilizados, a HTR não pode contar com a padronização do formato das letras por conta da multiplicidade de formas da escrita humana, mesmo se tratando de uma mesma caligrafia. Para tanto, as soluções de reconhecimento de manuscrito devem ser bem mais elaboradas, o que implica na criação de modelos de manuscritos, baseados em treinamento de inteligência artificial.
Em 2013 as iniciativas de Mühlberger tomaram a forma de um projeto que vislumbrava efetivamente o reconhecimento de textos manuscritos. Era o projeto tranScriptorium, que foi a base do que posteriormente se tornou o Transkribus, para o que foi fundamental a parceria com a Universidade de Valência. A ferramenta foi desenvolvida dentro do projeto original até 2016, quando foi criada a cooperativa READ, que desde então é responsável por sua manutenção e desenvolvimento de novas funcionalidades.
O Transkribus é certamente uma boa ferramenta, belo como o canto das sereias. O problema sempre é o risco de naufrágio. A tecnologia HTR não pode ser entendida como uma ferramenta neutra e, muito menos, descontextualizada de um mundo onde o grande volume de dados é sedutor. Por si só, o Transkribus não é uma ferramenta que necessariamente alimente uma obsessão empirista, mas ele certamente contribui para isso. 
Tal como no Venice Time Machine, a grande maioria dos textos que apresentam o projeto e seus outputs é fortemente tecnicista. Outra parte expressiva é composta pela descrição dos benefícios e o elenco do potencial público consumidor, em que historiadores e grande público estão lado a lado, como se a leitura das fontes antigas fosse a mesma ou muito próxima. Não há uma discussão sobre a visão de mundo que orienta a leitura ou a ambição de ter tanto material para análise. Ter a possibilidade de buscar dados em milhões de documentos antigos parece uma necessidade inquestionável.
A voracidade por documentos nem sempre é sinônimo de um empirismo voraz. Para contar a história de pessoas simples, sobre as quais poucos documentos foram gerados, precisamos ler e descartar milhares de potenciais documentos que poderiam - talvez - mencionar aquela pessoa. Consultamos muitos, utilizamos poucos. Isso ocorre porque as fontes sobre as pessoas do passado não foram geradas de modo homogêneo. Elas são completamente desiguais e refletem, no agregado do que foi produzido e do que restou, escolhas associadas com os poderes de cada tempo. Pessoas comuns são pouco descritas nas fontes e é razoável contar a história delas. Para tanto, precisamos observar todos os documentos possíveis.
A preocupação com a inclusão de pessoas menos aparentes nas fontes históricas não é, contudo, a tônica do Transkribus. Não há qualquer discussão sobre a seletividade da memória e se, ao fim e ao cabo, esse incremento empírico não vai acabar, finalmente, aumentando a desigualdade já existente na narrativa histórica, dando ainda mais importância a quem já é muito conhecido. A tônica do projeto está centrada numa metáfora: unlock the past, desbloquear, destravar ou abrir o passado. Esse é o mote do projeto, apresentado na página principal e na publicidade da ferramenta, o qual sugere que o passado estaria trancado. Sendo uma ferramenta de reconhecimento de caracteres, fica a impressão de que basta ler todos os documentos para liberá-lo. O passado está preso na paleografia.
A noção de unlock aparece em outros contextos na página do projeto. Ao apresentar o conjunto de ferramentas associadas ao Transkribus, a metáfora reaparece, com uma ligeira variação: unlock history. São as Features to unlock history, descritas nas sequencia: AI Text Recognition; Custom AI Training; Field & Table Recognition; Powerful Text Editor; Publishing & Search Tool. A observação histórica se resume a abrir documentos e fazer buscas por palavras-chave em fontes com texto já reconhecido. A ideia da teoria como uma importante ferramenta de decifração não aparece aqui. A maré dos dados nos guiará.
A noção de unlock history aparece reforçada nas palavras do criador do Transkribus, Günter Mühlberger, em uma breve entrevista com sua equipe em 2023. Segundo ele there are still so many interesting documents out there waiting to be discovered: Exploring them with HTR will be a big boost to historical research. Os documentos estão apenas esperando sua descoberta. E o encontro com o documento será um grande incremento na pesquisa em história. Não é preciso insistir que se trata de uma concepção de história arraigada em certo empirismo do século XIX. É o uso da inteligência artificial com uma imaginação romântica.

Sobre o neo-positivismo digital
As histórias do “Venice Time Machine” e do “Transkribus” não são casos isolados. São projetos próximos do campo dos historiadores e talvez por isso nos pareçam mais interessantes. Nossa vida digital cotidiana está marcada por “soluções” digitais que organizam milhões de dados e nos oferecem respostas tidas como “neutras” e aceitáveis, sem qualquer comprometimento político ou teórico. Ferramentas como os buscadores são um exemplo perfeito disso. Todos os dias utilizamos buscadores para encontrar informação, sem ter a menor ideia sobre as decisões que aquelas ferramentas tomam (automaticamente, mas criadas por um cérebro humano) para hierarquizar as respostas, excluir ou incluir itens e muitas outras variáveis. De Certeau nos lembrava, em 1972, que a seleção dos dados é o primeiro momento em que a teoria opera. O que fazer agora, quando um algoritmo seleciona os materiais com os quais lidamos, não apenas na pesquisa, mas também em nossas vidas? 
Par sa puissance et son efficacité, par son ambiguïté, Google neutralise le sens critique qui nous permettrait de garder à l’esprit que lorsque nous y cherchons une information, le moteur de recherche mondial avance une représentation particulière de la réalité pour nous répondre, et non la réalité elle-même.

Esse apelo à técnica como resposta primordial aos problemas dos historiadores não é privilégio de engenheiros que criam motores de busca, transcritores automáticos ou “modelos multidimensionais” da evolução de um dado lugar ao longo do tempo. Já penetrou profundamente na nossa disciplina, não somente pela amplitude (cada vez maior) das ferramentas digitais em nosso cotidiano, mas também pela vertigem na observação dos acervos online cada vez mais avassaladores, sejam aqueles nascidos digitais como os agora digitalizados. Encontramos  uma grande euforia de muitos historiadores diante do big data e das soluções técnicas. Há quem defenda que o uso de grandes volumes de dados, aliado a novas formas de visualização, pode ser disruptivo e dar início a novas epistemologias.
A programação tem se tornado uma solução bastante disponível como ferramenta de pesquisa, e estamos assistindo ao surgimento de diversos cursos não simplesmente de humanidades digitais, mas de História Digital propriamente dita. A crítica não pode se bastar ao fato de as ditas “humanidades” serem de difícil classificação, de saber até que ponto estas disciplinas compartilham, de fato, epistemologias e práticas. Ela precisa discutir os limites da técnica.  
Um exemplo dessa abordagem foi uma obra de 2014, The History Manifesto, que teve grande repercussão e gerou um forte debate. A maior parte do debate realizado se restringiu à discussão sobre a longue durée apresentada por Guldi e Armitage, seus autores. Segundo eles, novas pesquisas em história com maior abrangência cronológica teriam um efeito benéfico após anos de pesquisas de tempo curto, as quais seriam responsáveis, dentre outros problemas, por um afastamento da história de um potencial grande público e pela perda de relevância dos historiadores como figuras importantes na formação da opinião pública. Mas um aspecto foi menos debatido: o uso de big data, defendido pelos autores, a partir de ferramentas digitais. A defesa do big data aparece, agora, como uma solução para velhos problemas.
Guldi e Armitage não estão sozinhos. Temos uma profusão de revistas especializadas que vêm surgindo sobre o tema das humanidades digitais e da história digital. Não são publicações acríticas, pelo contrário, mas o espaço reservado para a discussão teórica é muito reduzido em comparação com a potencial difusão de “receitas de bolo” digitais. Em 2007, surge o “Digital Humanities Quarterly”. Em 2011, aparece o “Journal of Digital Humanities”. Em 2015, era a vez do “Journal of Open Humanities Data”. Em 2017, aparecia uma publicação em língua italiana, o “Umanistica Digitale”, criado pela também recente “Associazione per l’Informatica Umanistica e la Cultura Digitale” (2011). Em 2019, o “International Journal of Digital Humanities”. Em 2020 surgiam duas novas publicações: a francesa “Humanités numériques” e a italiana “Magazén: international journal for digital and public humanities”. Estou considerando aqui apenas publicações com foco integral no tema, sem considerar as dezenas de dossiers que foram lançados.
Desde 2010, há um surto de publicações técnicas voltadas para a pesquisa em humanidades. Um dos primeiros exemplos é “Macroanalysis: digital methods and literary history”, de Matthew Jockers (2011). Em 2014, Folgert Karsdorp apresentou uma edição virtual (em um “notebook virtual”) de um curso da linguagem de programação python, com  “Python Programming for the Humanities”, que gerou a edição posterior de “Humanities data analysis: case studies with Python” (2021), em colaboração com Mike Kestemont e Allen Riddell. Em 2018, Brian Kokensparger lançava “Guide to Programming for the Digital Humanities”, também focado na linguagem python. Em 2020, Jemielniak lançava seu “Thick big data: doing digital social sciences”, demonstrando que a voga das humanidades digitais andava mesmo na direção dos chamados big data. Também há uma grande difusão de cursos de verão, mestrados e doutorados erguidos sobre essa nova temática febril.
A tônica de todos estes cursos e publicações é fundamentalmente centrada na técnica como ferramenta geral para grandes volumes de dados. A discussão teórica, além de ter pouco apelo, é geralmente tratada como um problema individual, o que não seria de todo ruim, se o debate existisse. O problema, como dito, é que todo esse movimento ocorre em paralelo a outro grave processo: a negação da teoria. Essa é a denúncia que faz Pierre Mounier ao analisar um texto como o de Chris Anderson, The End of Theory, no qual o autor explicitamente avalia que a grande quantidade de dados cada vez mais acessível tornará, em breve, o método científico obsoleto. Não se trata de uma afirmação desconectada da realidade e do ambiente no qual surgiram todas aquelas obras que fizemos referência - ou mesmo aqueles cursos técnicos. O projeto “Time Machine” não tinha uma proposta estruturalmente distante dessa perspectiva. 
A proposta de Anderson não é uma posição isolada. Há um grande número de engenheiros, matemáticos e estatísticos que apostam na capacidade das ferramentas estatísticas, operando através de correlações, de gerar conhecimento novo. A partir de certos padrões, seria possível obter insights born from the data em uma nova era na qual the volume of data, accompanied by techniques that can reveal their inherent truth, enables data to speak for themselves free of theory.
Mounier centra seu foco em outro caso, o dos “culturomics”, como o Google Ngram Viewer e o muito recentemente lançado Gallicagram, nos quais milhões de livros são organizados e tornados pesquisáveis a partir de unidades básicas de texto, os ngram, que permitiriam identificar a frequência do uso de certos termos ao longo de certos períodos de tempo em corpora textuais imensos. Mounier discute o próprio exemplo dado por Michel et al, o caso do termo “Chagall” em milhões de livros de língua alemã, dando ênfase para a queda deste termo no período do nazismo, quando a referência a artistas judeus, como Chagall, era restrita. Mounier destaca que essa associação só foi possível graças não ao grande volume de dados do Google Ngram Viewer, mas do conhecimento prévio feito com trabalho não necessariamente digital ou quantitativo. Como faríamos para interpretar achados estatísticos de processos sobre os quais não temos a menor ideia? Como interpretar, por exemplo, uma possível queda nos registros de compra e venda em cartórios de Veneza (para retomar o caso anterior)? Como uma queda nos negócios ou nos registros de negócios, apenas para dar duas possibilidades bem simples? 
Ao fim e ao cabo, a análise de big data não permite qualquer crítica documental, pois não temos a menor ideia de como ela foi efetivamente produzida. Entender a existência de um documento histórico, sua criação e sua preservação ao longo do tempo, implica elucidar as diversas seleções realizadas socialmente ao longo do tempo, fazendo de algumas coisas lembradas e de outras tantas esquecidas. Os Arquivos, assim como os livros nas bibliotecas (para retomar os casos do Google Ngram Viewer e do Venice Time Machine) não são amostras representativas do mundo ou do passado, mas construções sociais geradas por uma sociedade em transformação. Como disse Mounier, “Ce n’est pas l’absence d’exhaustivité des données qui compte, c’est le fait que leur constitution est le résultat d’une intention humaine.”
A ameaça do big data é bastante visível se pensamos, como foi feito aqui, como uma ameaça ao método científico e à noção de teoria, mais especificamente. Por trás disso tudo, existe uma noção bastante difusa entre os historiadores, que é a separação entre teoria e técnica. Essa noção fora a base da experiência do “Venice Time Machine”, mas é muito mais difundida. Ela está presente em todos aqueles manuais de programação para historiadores e cursos que fizemos referência. Ela é finalmente aceita, pois muitos historiadores terceirizam suas atividades de levantamento documental e, mais recentemente, de processamento de dados com o uso de ferramentas digitais. Adotar um software que faça o trabalho parece algo absolutamente condizente com a pesquisa tal como ela sempre foi feita. Nisso, não há nenhum problema. Os problemas se colocam quando não sabemos ao certo o que o software faz.